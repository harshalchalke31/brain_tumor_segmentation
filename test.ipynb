{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from src.utils import BrainTumorSegmentationDataset, UNetTester\n",
    "from pathlib import Path\n",
    "from src.model1 import UNet\n",
    "from src.model2 import UNetR2D\n",
    "from src.model3 import MobileNetV3UNet\n",
    "from src.train import train_UNet\n",
    "from src.config.configuration import ConfigurationManager\n",
    "import gc\n",
    "\n",
    "## Global parameters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "H,W = 256,256\n",
    "torch.manual_seed(42)\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "\n",
    "dataset_path_1 = Path(\"./data/archive/\")\n",
    "model_path_1 = Path('./models/model1/best_model2.pt')\n",
    "model_path_2 = Path('./models/model2/best_model3.pt')\n",
    "model_path_3 = Path('./models/model3/best_model3.pt')\n",
    "log_path_1 = Path('./logs/test_log_unet_2.csv')\n",
    "log_path_2 = Path('./logs/test_log_transunet_3.csv')\n",
    "log_path_3 = Path('./logs/test_log_mobilenetv3_3.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((H, W)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_dataset = BrainTumorSegmentationDataset(\n",
    "    image_dir= os.path.join(dataset_path_1,'test','images'),\n",
    "    mask_dir=os.path.join(dataset_path_1,'test','masks'),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader_1 = DataLoader(test_dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-sample and average metrics saved to: logs\\test_log_unet_2.csv\n",
      "Tested on 16 images.\n",
      "Average metrics: {'dice': 0.8261965066194534, 'jaccard': 0.7060334607958794, 'precision': 0.847361046820879, 'recall': 0.814082320779562, 'f1': 0.8261915184557438}\n"
     ]
    }
   ],
   "source": [
    "model_1 = UNet()\n",
    "output_dir = './logs'\n",
    "tester = UNetTester(\n",
    "    model=model_1,\n",
    "    device=device,\n",
    "    model_path=model_path_1,\n",
    "    test_loader=test_loader_1, \n",
    "    log_path=log_path_1\n",
    ")\n",
    "\n",
    "# 6) Evaluate the model -> compute and save metrics\n",
    "all_metrics, avg_metrics = tester.evaluate_model()\n",
    "print(f\"Tested on {len(all_metrics)} images.\")\n",
    "print(\"Average metrics:\", avg_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-sample and average metrics saved to: logs\\test_log_transunet_3.csv\n",
      "Tested on 16 images.\n",
      "Average metrics: {'dice': 0.6407659780234098, 'jaccard': 0.479664733633399, 'precision': 0.7575067766010761, 'recall': 0.565256156027317, 'f1': 0.6407611425966024}\n"
     ]
    }
   ],
   "source": [
    "configuration = ConfigurationManager()\n",
    "config = configuration.get_UNetR_params()\n",
    "model_2 = UNetR2D(config=config)\n",
    "tester = UNetTester(\n",
    "    model=model_2,\n",
    "    device=device,\n",
    "    model_path=model_path_2,\n",
    "    test_loader=test_loader_1, \n",
    "    log_path=log_path_2\n",
    ")\n",
    "\n",
    "# 6) Evaluate the model -> compute and save metrics\n",
    "all_metrics, avg_metrics = tester.evaluate_model()\n",
    "print(f\"Tested on {len(all_metrics)} images.\")\n",
    "print(\"Average metrics:\", avg_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\python\\mainenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Projects\\python\\mainenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-sample and average metrics saved to: logs\\test_log_mobilenetv3_3.csv\n",
      "Tested on 16 images.\n",
      "Average metrics: {'dice': 0.8603424914181232, 'jaccard': 0.7565244287252426, 'precision': 0.8415287807583809, 'recall': 0.8844862468540668, 'f1': 0.8603375107049942}\n"
     ]
    }
   ],
   "source": [
    "model_3 = MobileNetV3UNet()\n",
    "tester = UNetTester(\n",
    "    model=model_3,\n",
    "    device=device,\n",
    "    model_path=model_path_3,\n",
    "    test_loader=test_loader_1, \n",
    "    log_path=log_path_3\n",
    ")\n",
    "\n",
    "# 6) Evaluate the model -> compute and save metrics\n",
    "all_metrics, avg_metrics = tester.evaluate_model()\n",
    "print(f\"Tested on {len(all_metrics)} images.\")\n",
    "print(\"Average metrics:\", avg_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
