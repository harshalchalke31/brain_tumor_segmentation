digraph {
	graph [size="70.35,70.35"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2266579431600 [label="
 (1, 1, 256, 256)" fillcolor=darkolivegreen1]
	2266579260592 [label=ConvolutionBackward0]
	2266579260160 -> 2266579260592
	2266579260160 [label=ReluBackward0]
	2266579260352 -> 2266579260160
	2266579260352 [label=CudnnBatchNormBackward0]
	2266579260016 -> 2266579260352
	2266579260016 [label=ConvolutionBackward0]
	2266579259680 -> 2266579260016
	2266579259680 [label=ReluBackward0]
	2266579259488 -> 2266579259680
	2266579259488 [label=CudnnBatchNormBackward0]
	2266579259296 -> 2266579259488
	2266579259296 [label=ConvolutionBackward0]
	2266579259056 -> 2266579259296
	2266579259056 [label=CatBackward0]
	2266579261024 -> 2266579259056
	2266579261024 [label=ConvolutionBackward0]
	2266579261168 -> 2266579261024
	2266579261168 [label=ReluBackward0]
	2266579261360 -> 2266579261168
	2266579261360 [label=CudnnBatchNormBackward0]
	2266579261456 -> 2266579261360
	2266579261456 [label=ConvolutionBackward0]
	2266579261648 -> 2266579261456
	2266579261648 [label=ReluBackward0]
	2266579261840 -> 2266579261648
	2266579261840 [label=CudnnBatchNormBackward0]
	2266579261936 -> 2266579261840
	2266579261936 [label=ConvolutionBackward0]
	2266579262128 -> 2266579261936
	2266579262128 [label=CatBackward0]
	2266579262320 -> 2266579262128
	2266579262320 [label=ConvolutionBackward0]
	2266579262464 -> 2266579262320
	2266579262464 [label=ReluBackward0]
	2266579262656 -> 2266579262464
	2266579262656 [label=CudnnBatchNormBackward0]
	2266579262752 -> 2266579262656
	2266579262752 [label=ConvolutionBackward0]
	2266579262944 -> 2266579262752
	2266579262944 [label=ReluBackward0]
	2266579263136 -> 2266579262944
	2266579263136 [label=CudnnBatchNormBackward0]
	2266579263232 -> 2266579263136
	2266579263232 [label=ConvolutionBackward0]
	2266579263424 -> 2266579263232
	2266579263424 [label=CatBackward0]
	2266579263616 -> 2266579263424
	2266579263616 [label=ConvolutionBackward0]
	2266579263760 -> 2266579263616
	2266579263760 [label=ReluBackward0]
	2266579263952 -> 2266579263760
	2266579263952 [label=CudnnBatchNormBackward0]
	2266579264048 -> 2266579263952
	2266579264048 [label=ConvolutionBackward0]
	2266579264240 -> 2266579264048
	2266579264240 [label=ReluBackward0]
	2266579264432 -> 2266579264240
	2266579264432 [label=CudnnBatchNormBackward0]
	2266579264528 -> 2266579264432
	2266579264528 [label=ConvolutionBackward0]
	2266579264720 -> 2266579264528
	2266579264720 [label=CatBackward0]
	2266579264912 -> 2266579264720
	2266579264912 [label=ConvolutionBackward0]
	2266579265056 -> 2266579264912
	2266579265056 [label=MulBackward0]
	2266579265248 -> 2266579265056
	2266579265248 [label=ReluBackward0]
	2266579265344 -> 2266579265248
	2266579265344 [label=CudnnBatchNormBackward0]
	2266579265440 -> 2266579265344
	2266579265440 [label=ConvolutionBackward0]
	2266579265632 -> 2266579265440
	2266579265632 [label=ReluBackward0]
	2266579265824 -> 2266579265632
	2266579265824 [label=CudnnBatchNormBackward0]
	2266579265920 -> 2266579265824
	2266579265920 [label=ConvolutionBackward0]
	2266579266112 -> 2266579265920
	2266579266112 [label=MaxPool2DWithIndicesBackward0]
	2266579264864 -> 2266579266112
	2266579264864 [label=ReluBackward0]
	2266579266352 -> 2266579264864
	2266579266352 [label=CudnnBatchNormBackward0]
	2266579266448 -> 2266579266352
	2266579266448 [label=ConvolutionBackward0]
	2266579266640 -> 2266579266448
	2266579266640 [label=ReluBackward0]
	2266579266832 -> 2266579266640
	2266579266832 [label=CudnnBatchNormBackward0]
	2266579266928 -> 2266579266832
	2266579266928 [label=ConvolutionBackward0]
	2266579267120 -> 2266579266928
	2266579267120 [label=MaxPool2DWithIndicesBackward0]
	2266579263568 -> 2266579267120
	2266579263568 [label=ReluBackward0]
	2266579267360 -> 2266579263568
	2266579267360 [label=CudnnBatchNormBackward0]
	2266579267456 -> 2266579267360
	2266579267456 [label=ConvolutionBackward0]
	2266579267648 -> 2266579267456
	2266579267648 [label=ReluBackward0]
	2266579267840 -> 2266579267648
	2266579267840 [label=CudnnBatchNormBackward0]
	2266579267936 -> 2266579267840
	2266579267936 [label=ConvolutionBackward0]
	2266579268128 -> 2266579267936
	2266579268128 [label=MaxPool2DWithIndicesBackward0]
	2266579262272 -> 2266579268128
	2266579262272 [label=ReluBackward0]
	2266579268368 -> 2266579262272
	2266579268368 [label=CudnnBatchNormBackward0]
	2266579268464 -> 2266579268368
	2266579268464 [label=ConvolutionBackward0]
	2266579268656 -> 2266579268464
	2266579268656 [label=ReluBackward0]
	2266579268848 -> 2266579268656
	2266579268848 [label=CudnnBatchNormBackward0]
	2266579268944 -> 2266579268848
	2266579268944 [label=ConvolutionBackward0]
	2266579269136 -> 2266579268944
	2266579269136 [label=MaxPool2DWithIndicesBackward0]
	2266579260976 -> 2266579269136
	2266579260976 [label=ReluBackward0]
	2266579269376 -> 2266579260976
	2266579269376 [label=CudnnBatchNormBackward0]
	2266579269472 -> 2266579269376
	2266579269472 [label=ConvolutionBackward0]
	2266579269664 -> 2266579269472
	2266579269664 [label=ReluBackward0]
	2266579269856 -> 2266579269664
	2266579269856 [label=CudnnBatchNormBackward0]
	2266579269952 -> 2266579269856
	2266579269952 [label=ConvolutionBackward0]
	2266579270144 -> 2266579269952
	2266579168400 [label="encoder1.conv.conv_block.0.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	2266579168400 -> 2266579270144
	2266579270144 [label=AccumulateGrad]
	2266579270096 -> 2266579269952
	2266579168592 [label="encoder1.conv.conv_block.0.bias
 (64)" fillcolor=lightblue]
	2266579168592 -> 2266579270096
	2266579270096 [label=AccumulateGrad]
	2266579269904 -> 2266579269856
	2266579168688 [label="encoder1.conv.conv_block.1.weight
 (64)" fillcolor=lightblue]
	2266579168688 -> 2266579269904
	2266579269904 [label=AccumulateGrad]
	2266579269760 -> 2266579269856
	2266579168784 [label="encoder1.conv.conv_block.1.bias
 (64)" fillcolor=lightblue]
	2266579168784 -> 2266579269760
	2266579269760 [label=AccumulateGrad]
	2266579269616 -> 2266579269472
	2266579169168 [label="encoder1.conv.conv_block.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2266579169168 -> 2266579269616
	2266579269616 [label=AccumulateGrad]
	2266579269568 -> 2266579269472
	2266579169264 [label="encoder1.conv.conv_block.3.bias
 (64)" fillcolor=lightblue]
	2266579169264 -> 2266579269568
	2266579269568 [label=AccumulateGrad]
	2266579269424 -> 2266579269376
	2266579169360 [label="encoder1.conv.conv_block.4.weight
 (64)" fillcolor=lightblue]
	2266579169360 -> 2266579269424
	2266579269424 [label=AccumulateGrad]
	2266579269280 -> 2266579269376
	2266579169456 [label="encoder1.conv.conv_block.4.bias
 (64)" fillcolor=lightblue]
	2266579169456 -> 2266579269280
	2266579269280 [label=AccumulateGrad]
	2266579269088 -> 2266579268944
	2266579169936 [label="encoder2.conv.conv_block.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2266579169936 -> 2266579269088
	2266579269088 [label=AccumulateGrad]
	2266579269040 -> 2266579268944
	2266579170032 [label="encoder2.conv.conv_block.0.bias
 (128)" fillcolor=lightblue]
	2266579170032 -> 2266579269040
	2266579269040 [label=AccumulateGrad]
	2266579268896 -> 2266579268848
	2266579170128 [label="encoder2.conv.conv_block.1.weight
 (128)" fillcolor=lightblue]
	2266579170128 -> 2266579268896
	2266579268896 [label=AccumulateGrad]
	2266579268752 -> 2266579268848
	2266579170224 [label="encoder2.conv.conv_block.1.bias
 (128)" fillcolor=lightblue]
	2266579170224 -> 2266579268752
	2266579268752 [label=AccumulateGrad]
	2266579268608 -> 2266579268464
	2266579170608 [label="encoder2.conv.conv_block.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2266579170608 -> 2266579268608
	2266579268608 [label=AccumulateGrad]
	2266579268560 -> 2266579268464
	2266579170704 [label="encoder2.conv.conv_block.3.bias
 (128)" fillcolor=lightblue]
	2266579170704 -> 2266579268560
	2266579268560 [label=AccumulateGrad]
	2266579268416 -> 2266579268368
	2266579170800 [label="encoder2.conv.conv_block.4.weight
 (128)" fillcolor=lightblue]
	2266579170800 -> 2266579268416
	2266579268416 [label=AccumulateGrad]
	2266579268272 -> 2266579268368
	2266579170896 [label="encoder2.conv.conv_block.4.bias
 (128)" fillcolor=lightblue]
	2266579170896 -> 2266579268272
	2266579268272 [label=AccumulateGrad]
	2266579268080 -> 2266579267936
	2266579171280 [label="encoder3.conv.conv_block.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2266579171280 -> 2266579268080
	2266579268080 [label=AccumulateGrad]
	2266579268032 -> 2266579267936
	2266579171376 [label="encoder3.conv.conv_block.0.bias
 (256)" fillcolor=lightblue]
	2266579171376 -> 2266579268032
	2266579268032 [label=AccumulateGrad]
	2266579267888 -> 2266579267840
	2266579171472 [label="encoder3.conv.conv_block.1.weight
 (256)" fillcolor=lightblue]
	2266579171472 -> 2266579267888
	2266579267888 [label=AccumulateGrad]
	2266579267744 -> 2266579267840
	2266579171568 [label="encoder3.conv.conv_block.1.bias
 (256)" fillcolor=lightblue]
	2266579171568 -> 2266579267744
	2266579267744 [label=AccumulateGrad]
	2266579267600 -> 2266579267456
	2266579171952 [label="encoder3.conv.conv_block.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2266579171952 -> 2266579267600
	2266579267600 [label=AccumulateGrad]
	2266579267552 -> 2266579267456
	2266579172048 [label="encoder3.conv.conv_block.3.bias
 (256)" fillcolor=lightblue]
	2266579172048 -> 2266579267552
	2266579267552 [label=AccumulateGrad]
	2266579267408 -> 2266579267360
	2266579172144 [label="encoder3.conv.conv_block.4.weight
 (256)" fillcolor=lightblue]
	2266579172144 -> 2266579267408
	2266579267408 [label=AccumulateGrad]
	2266579267264 -> 2266579267360
	2266579172240 [label="encoder3.conv.conv_block.4.bias
 (256)" fillcolor=lightblue]
	2266579172240 -> 2266579267264
	2266579267264 [label=AccumulateGrad]
	2266579267072 -> 2266579266928
	2266579172624 [label="encoder4.conv.conv_block.0.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2266579172624 -> 2266579267072
	2266579267072 [label=AccumulateGrad]
	2266579267024 -> 2266579266928
	2266579172720 [label="encoder4.conv.conv_block.0.bias
 (512)" fillcolor=lightblue]
	2266579172720 -> 2266579267024
	2266579267024 [label=AccumulateGrad]
	2266579266880 -> 2266579266832
	2266579172816 [label="encoder4.conv.conv_block.1.weight
 (512)" fillcolor=lightblue]
	2266579172816 -> 2266579266880
	2266579266880 [label=AccumulateGrad]
	2266579266736 -> 2266579266832
	2266579172912 [label="encoder4.conv.conv_block.1.bias
 (512)" fillcolor=lightblue]
	2266579172912 -> 2266579266736
	2266579266736 [label=AccumulateGrad]
	2266579266592 -> 2266579266448
	2266579173296 [label="encoder4.conv.conv_block.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2266579173296 -> 2266579266592
	2266579266592 [label=AccumulateGrad]
	2266579266496 -> 2266579266448
	2266579419216 [label="encoder4.conv.conv_block.3.bias
 (512)" fillcolor=lightblue]
	2266579419216 -> 2266579266496
	2266579266496 [label=AccumulateGrad]
	2266579266400 -> 2266579266352
	2266579419312 [label="encoder4.conv.conv_block.4.weight
 (512)" fillcolor=lightblue]
	2266579419312 -> 2266579266400
	2266579266400 [label=AccumulateGrad]
	2266579266256 -> 2266579266352
	2266579419408 [label="encoder4.conv.conv_block.4.bias
 (512)" fillcolor=lightblue]
	2266579419408 -> 2266579266256
	2266579266256 [label=AccumulateGrad]
	2266579266064 -> 2266579265920
	2266579419792 [label="bottleneck.conv_block.0.weight
 (1024, 512, 3, 3)" fillcolor=lightblue]
	2266579419792 -> 2266579266064
	2266579266064 [label=AccumulateGrad]
	2266579266016 -> 2266579265920
	2266579419888 [label="bottleneck.conv_block.0.bias
 (1024)" fillcolor=lightblue]
	2266579419888 -> 2266579266016
	2266579266016 [label=AccumulateGrad]
	2266579265872 -> 2266579265824
	2266579419984 [label="bottleneck.conv_block.1.weight
 (1024)" fillcolor=lightblue]
	2266579419984 -> 2266579265872
	2266579265872 [label=AccumulateGrad]
	2266579265728 -> 2266579265824
	2266579420080 [label="bottleneck.conv_block.1.bias
 (1024)" fillcolor=lightblue]
	2266579420080 -> 2266579265728
	2266579265728 [label=AccumulateGrad]
	2266579265584 -> 2266579265440
	2266579420464 [label="bottleneck.conv_block.3.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	2266579420464 -> 2266579265584
	2266579265584 [label=AccumulateGrad]
	2266579265536 -> 2266579265440
	2266579420560 [label="bottleneck.conv_block.3.bias
 (1024)" fillcolor=lightblue]
	2266579420560 -> 2266579265536
	2266579265536 [label=AccumulateGrad]
	2266579265392 -> 2266579265344
	2266579420656 [label="bottleneck.conv_block.4.weight
 (1024)" fillcolor=lightblue]
	2266579420656 -> 2266579265392
	2266579265392 [label=AccumulateGrad]
	2266579265152 -> 2266579265344
	2266579420752 [label="bottleneck.conv_block.4.bias
 (1024)" fillcolor=lightblue]
	2266579420752 -> 2266579265152
	2266579265152 [label=AccumulateGrad]
	2266579265008 -> 2266579264912
	2266579421136 [label="decoder1.upconv.weight
 (1024, 512, 2, 2)" fillcolor=lightblue]
	2266579421136 -> 2266579265008
	2266579265008 [label=AccumulateGrad]
	2266579264960 -> 2266579264912
	2266579421232 [label="decoder1.upconv.bias
 (512)" fillcolor=lightblue]
	2266579421232 -> 2266579264960
	2266579264960 [label=AccumulateGrad]
	2266579264864 -> 2266579264720
	2266579264672 -> 2266579264528
	2266579421328 [label="decoder1.conv.conv_block.0.weight
 (512, 1024, 3, 3)" fillcolor=lightblue]
	2266579421328 -> 2266579264672
	2266579264672 [label=AccumulateGrad]
	2266579264624 -> 2266579264528
	2266579421424 [label="decoder1.conv.conv_block.0.bias
 (512)" fillcolor=lightblue]
	2266579421424 -> 2266579264624
	2266579264624 [label=AccumulateGrad]
	2266579264480 -> 2266579264432
	2266579421520 [label="decoder1.conv.conv_block.1.weight
 (512)" fillcolor=lightblue]
	2266579421520 -> 2266579264480
	2266579264480 [label=AccumulateGrad]
	2266579264336 -> 2266579264432
	2266579421616 [label="decoder1.conv.conv_block.1.bias
 (512)" fillcolor=lightblue]
	2266579421616 -> 2266579264336
	2266579264336 [label=AccumulateGrad]
	2266579264192 -> 2266579264048
	2266579422000 [label="decoder1.conv.conv_block.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2266579422000 -> 2266579264192
	2266579264192 [label=AccumulateGrad]
	2266579264144 -> 2266579264048
	2266579422096 [label="decoder1.conv.conv_block.3.bias
 (512)" fillcolor=lightblue]
	2266579422096 -> 2266579264144
	2266579264144 [label=AccumulateGrad]
	2266579264000 -> 2266579263952
	2266579422192 [label="decoder1.conv.conv_block.4.weight
 (512)" fillcolor=lightblue]
	2266579422192 -> 2266579264000
	2266579264000 [label=AccumulateGrad]
	2266579263856 -> 2266579263952
	2266579422288 [label="decoder1.conv.conv_block.4.bias
 (512)" fillcolor=lightblue]
	2266579422288 -> 2266579263856
	2266579263856 [label=AccumulateGrad]
	2266579263712 -> 2266579263616
	2266579422672 [label="decoder2.upconv.weight
 (512, 256, 2, 2)" fillcolor=lightblue]
	2266579422672 -> 2266579263712
	2266579263712 [label=AccumulateGrad]
	2266579263664 -> 2266579263616
	2266579422768 [label="decoder2.upconv.bias
 (256)" fillcolor=lightblue]
	2266579422768 -> 2266579263664
	2266579263664 [label=AccumulateGrad]
	2266579263568 -> 2266579263424
	2266579263376 -> 2266579263232
	2266579422864 [label="decoder2.conv.conv_block.0.weight
 (256, 512, 3, 3)" fillcolor=lightblue]
	2266579422864 -> 2266579263376
	2266579263376 [label=AccumulateGrad]
	2266579263328 -> 2266579263232
	2266579422960 [label="decoder2.conv.conv_block.0.bias
 (256)" fillcolor=lightblue]
	2266579422960 -> 2266579263328
	2266579263328 [label=AccumulateGrad]
	2266579263184 -> 2266579263136
	2266579423056 [label="decoder2.conv.conv_block.1.weight
 (256)" fillcolor=lightblue]
	2266579423056 -> 2266579263184
	2266579263184 [label=AccumulateGrad]
	2266579263040 -> 2266579263136
	2266579423152 [label="decoder2.conv.conv_block.1.bias
 (256)" fillcolor=lightblue]
	2266579423152 -> 2266579263040
	2266579263040 [label=AccumulateGrad]
	2266579262896 -> 2266579262752
	2266579423536 [label="decoder2.conv.conv_block.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2266579423536 -> 2266579262896
	2266579262896 [label=AccumulateGrad]
	2266579262848 -> 2266579262752
	2266579423632 [label="decoder2.conv.conv_block.3.bias
 (256)" fillcolor=lightblue]
	2266579423632 -> 2266579262848
	2266579262848 [label=AccumulateGrad]
	2266579262704 -> 2266579262656
	2266579423728 [label="decoder2.conv.conv_block.4.weight
 (256)" fillcolor=lightblue]
	2266579423728 -> 2266579262704
	2266579262704 [label=AccumulateGrad]
	2266579262560 -> 2266579262656
	2266579423824 [label="decoder2.conv.conv_block.4.bias
 (256)" fillcolor=lightblue]
	2266579423824 -> 2266579262560
	2266579262560 [label=AccumulateGrad]
	2266579262416 -> 2266579262320
	2266579424208 [label="decoder3.upconv.weight
 (256, 128, 2, 2)" fillcolor=lightblue]
	2266579424208 -> 2266579262416
	2266579262416 [label=AccumulateGrad]
	2266579262368 -> 2266579262320
	2266579424304 [label="decoder3.upconv.bias
 (128)" fillcolor=lightblue]
	2266579424304 -> 2266579262368
	2266579262368 [label=AccumulateGrad]
	2266579262272 -> 2266579262128
	2266579262080 -> 2266579261936
	2266579424400 [label="decoder3.conv.conv_block.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	2266579424400 -> 2266579262080
	2266579262080 [label=AccumulateGrad]
	2266579262032 -> 2266579261936
	2266579424496 [label="decoder3.conv.conv_block.0.bias
 (128)" fillcolor=lightblue]
	2266579424496 -> 2266579262032
	2266579262032 [label=AccumulateGrad]
	2266579261888 -> 2266579261840
	2266579424592 [label="decoder3.conv.conv_block.1.weight
 (128)" fillcolor=lightblue]
	2266579424592 -> 2266579261888
	2266579261888 [label=AccumulateGrad]
	2266579261744 -> 2266579261840
	2266579424688 [label="decoder3.conv.conv_block.1.bias
 (128)" fillcolor=lightblue]
	2266579424688 -> 2266579261744
	2266579261744 [label=AccumulateGrad]
	2266579261600 -> 2266579261456
	2266579425072 [label="decoder3.conv.conv_block.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2266579425072 -> 2266579261600
	2266579261600 [label=AccumulateGrad]
	2266579261552 -> 2266579261456
	2266579425168 [label="decoder3.conv.conv_block.3.bias
 (128)" fillcolor=lightblue]
	2266579425168 -> 2266579261552
	2266579261552 [label=AccumulateGrad]
	2266579261408 -> 2266579261360
	2266579425264 [label="decoder3.conv.conv_block.4.weight
 (128)" fillcolor=lightblue]
	2266579425264 -> 2266579261408
	2266579261408 [label=AccumulateGrad]
	2266579261264 -> 2266579261360
	2266579425360 [label="decoder3.conv.conv_block.4.bias
 (128)" fillcolor=lightblue]
	2266579425360 -> 2266579261264
	2266579261264 [label=AccumulateGrad]
	2266579261120 -> 2266579261024
	2266579425744 [label="decoder4.upconv.weight
 (128, 64, 2, 2)" fillcolor=lightblue]
	2266579425744 -> 2266579261120
	2266579261120 [label=AccumulateGrad]
	2266579261072 -> 2266579261024
	2266579425840 [label="decoder4.upconv.bias
 (64)" fillcolor=lightblue]
	2266579425840 -> 2266579261072
	2266579261072 [label=AccumulateGrad]
	2266579260976 -> 2266579259056
	2266579259104 -> 2266579259296
	2266579425936 [label="decoder4.conv.conv_block.0.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	2266579425936 -> 2266579259104
	2266579259104 [label=AccumulateGrad]
	2266579259200 -> 2266579259296
	2266579426032 [label="decoder4.conv.conv_block.0.bias
 (64)" fillcolor=lightblue]
	2266579426032 -> 2266579259200
	2266579259200 [label=AccumulateGrad]
	2266579259344 -> 2266579259488
	2266579426128 [label="decoder4.conv.conv_block.1.weight
 (64)" fillcolor=lightblue]
	2266579426128 -> 2266579259344
	2266579259344 [label=AccumulateGrad]
	2266579259584 -> 2266579259488
	2266579426224 [label="decoder4.conv.conv_block.1.bias
 (64)" fillcolor=lightblue]
	2266579426224 -> 2266579259584
	2266579259584 [label=AccumulateGrad]
	2266579259728 -> 2266579260016
	2266579426608 [label="decoder4.conv.conv_block.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2266579426608 -> 2266579259728
	2266579259728 [label=AccumulateGrad]
	2266579259776 -> 2266579260016
	2266579426704 [label="decoder4.conv.conv_block.3.bias
 (64)" fillcolor=lightblue]
	2266579426704 -> 2266579259776
	2266579259776 [label=AccumulateGrad]
	2266579260064 -> 2266579260352
	2266579426800 [label="decoder4.conv.conv_block.4.weight
 (64)" fillcolor=lightblue]
	2266579426800 -> 2266579260064
	2266579260064 [label=AccumulateGrad]
	2266579260304 -> 2266579260352
	2266579426896 [label="decoder4.conv.conv_block.4.bias
 (64)" fillcolor=lightblue]
	2266579426896 -> 2266579260304
	2266579260304 [label=AccumulateGrad]
	2266579260208 -> 2266579260592
	2266579427280 [label="final_layer.weight
 (1, 64, 1, 1)" fillcolor=lightblue]
	2266579427280 -> 2266579260208
	2266579260208 [label=AccumulateGrad]
	2266579260256 -> 2266579260592
	2266579427376 [label="final_layer.bias
 (1)" fillcolor=lightblue]
	2266579427376 -> 2266579260256
	2266579260256 [label=AccumulateGrad]
	2266579260592 -> 2266579431600
}
