digraph {
	graph [size="70.35,70.35"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2075079700368 [label="
 (1, 1, 256, 256)" fillcolor=darkolivegreen1]
	2075079529888 [label=ConvolutionBackward0]
	2075079529456 -> 2075079529888
	2075079529456 [label=ReluBackward0]
	2075079529648 -> 2075079529456
	2075079529648 [label=CudnnBatchNormBackward0]
	2075079529312 -> 2075079529648
	2075079529312 [label=ConvolutionBackward0]
	2075079528976 -> 2075079529312
	2075079528976 [label=ReluBackward0]
	2075079528784 -> 2075079528976
	2075079528784 [label=CudnnBatchNormBackward0]
	2075079528592 -> 2075079528784
	2075079528592 [label=ConvolutionBackward0]
	2075079528352 -> 2075079528592
	2075079528352 [label=CatBackward0]
	2075079530320 -> 2075079528352
	2075079530320 [label=ConvolutionBackward0]
	2075079530464 -> 2075079530320
	2075079530464 [label=ReluBackward0]
	2075079530656 -> 2075079530464
	2075079530656 [label=CudnnBatchNormBackward0]
	2075079530752 -> 2075079530656
	2075079530752 [label=ConvolutionBackward0]
	2075079530944 -> 2075079530752
	2075079530944 [label=ReluBackward0]
	2075079531136 -> 2075079530944
	2075079531136 [label=CudnnBatchNormBackward0]
	2075079531232 -> 2075079531136
	2075079531232 [label=ConvolutionBackward0]
	2075079531424 -> 2075079531232
	2075079531424 [label=CatBackward0]
	2075079531616 -> 2075079531424
	2075079531616 [label=ConvolutionBackward0]
	2075079531760 -> 2075079531616
	2075079531760 [label=ReluBackward0]
	2075079531952 -> 2075079531760
	2075079531952 [label=CudnnBatchNormBackward0]
	2075079532048 -> 2075079531952
	2075079532048 [label=ConvolutionBackward0]
	2075079532240 -> 2075079532048
	2075079532240 [label=ReluBackward0]
	2075079532432 -> 2075079532240
	2075079532432 [label=CudnnBatchNormBackward0]
	2075079532528 -> 2075079532432
	2075079532528 [label=ConvolutionBackward0]
	2075079532720 -> 2075079532528
	2075079532720 [label=CatBackward0]
	2075079532912 -> 2075079532720
	2075079532912 [label=ConvolutionBackward0]
	2075079533056 -> 2075079532912
	2075079533056 [label=ReluBackward0]
	2075079533248 -> 2075079533056
	2075079533248 [label=CudnnBatchNormBackward0]
	2075079533344 -> 2075079533248
	2075079533344 [label=ConvolutionBackward0]
	2075079533536 -> 2075079533344
	2075079533536 [label=ReluBackward0]
	2075079533728 -> 2075079533536
	2075079533728 [label=CudnnBatchNormBackward0]
	2075079533824 -> 2075079533728
	2075079533824 [label=ConvolutionBackward0]
	2075079534016 -> 2075079533824
	2075079534016 [label=CatBackward0]
	2075079534208 -> 2075079534016
	2075079534208 [label=ConvolutionBackward0]
	2075079534352 -> 2075079534208
	2075079534352 [label=MulBackward0]
	2075079534544 -> 2075079534352
	2075079534544 [label=ReluBackward0]
	2075079534640 -> 2075079534544
	2075079534640 [label=CudnnBatchNormBackward0]
	2075079534736 -> 2075079534640
	2075079534736 [label=ConvolutionBackward0]
	2075079534928 -> 2075079534736
	2075079534928 [label=ReluBackward0]
	2075079535120 -> 2075079534928
	2075079535120 [label=CudnnBatchNormBackward0]
	2075079535216 -> 2075079535120
	2075079535216 [label=ConvolutionBackward0]
	2075079535408 -> 2075079535216
	2075079535408 [label=MaxPool2DWithIndicesBackward0]
	2075079534160 -> 2075079535408
	2075079534160 [label=ReluBackward0]
	2075079535648 -> 2075079534160
	2075079535648 [label=CudnnBatchNormBackward0]
	2075079535744 -> 2075079535648
	2075079535744 [label=ConvolutionBackward0]
	2075079535936 -> 2075079535744
	2075079535936 [label=ReluBackward0]
	2075079536128 -> 2075079535936
	2075079536128 [label=CudnnBatchNormBackward0]
	2075079536224 -> 2075079536128
	2075079536224 [label=ConvolutionBackward0]
	2075079536416 -> 2075079536224
	2075079536416 [label=MaxPool2DWithIndicesBackward0]
	2075079532864 -> 2075079536416
	2075079532864 [label=ReluBackward0]
	2075079536656 -> 2075079532864
	2075079536656 [label=CudnnBatchNormBackward0]
	2075079536752 -> 2075079536656
	2075079536752 [label=ConvolutionBackward0]
	2075079536944 -> 2075079536752
	2075079536944 [label=ReluBackward0]
	2075079537136 -> 2075079536944
	2075079537136 [label=CudnnBatchNormBackward0]
	2075079537232 -> 2075079537136
	2075079537232 [label=ConvolutionBackward0]
	2075079537424 -> 2075079537232
	2075079537424 [label=MaxPool2DWithIndicesBackward0]
	2075079531568 -> 2075079537424
	2075079531568 [label=ReluBackward0]
	2075079537664 -> 2075079531568
	2075079537664 [label=CudnnBatchNormBackward0]
	2075079537760 -> 2075079537664
	2075079537760 [label=ConvolutionBackward0]
	2075079537952 -> 2075079537760
	2075079537952 [label=ReluBackward0]
	2075079538144 -> 2075079537952
	2075079538144 [label=CudnnBatchNormBackward0]
	2075079538240 -> 2075079538144
	2075079538240 [label=ConvolutionBackward0]
	2075079538432 -> 2075079538240
	2075079538432 [label=MaxPool2DWithIndicesBackward0]
	2075079530272 -> 2075079538432
	2075079530272 [label=ReluBackward0]
	2075079538672 -> 2075079530272
	2075079538672 [label=CudnnBatchNormBackward0]
	2075079538768 -> 2075079538672
	2075079538768 [label=ConvolutionBackward0]
	2075079538960 -> 2075079538768
	2075079538960 [label=ReluBackward0]
	2075079539152 -> 2075079538960
	2075079539152 [label=CudnnBatchNormBackward0]
	2075079539248 -> 2075079539152
	2075079539248 [label=ConvolutionBackward0]
	2075079539440 -> 2075079539248
	2075079437264 [label="encoder1.conv.conv_block.0.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	2075079437264 -> 2075079539440
	2075079539440 [label=AccumulateGrad]
	2075079539392 -> 2075079539248
	2075079437456 [label="encoder1.conv.conv_block.0.bias
 (64)" fillcolor=lightblue]
	2075079437456 -> 2075079539392
	2075079539392 [label=AccumulateGrad]
	2075079539200 -> 2075079539152
	2075079437552 [label="encoder1.conv.conv_block.1.weight
 (64)" fillcolor=lightblue]
	2075079437552 -> 2075079539200
	2075079539200 [label=AccumulateGrad]
	2075079539056 -> 2075079539152
	2075079437648 [label="encoder1.conv.conv_block.1.bias
 (64)" fillcolor=lightblue]
	2075079437648 -> 2075079539056
	2075079539056 [label=AccumulateGrad]
	2075079538912 -> 2075079538768
	2075079438032 [label="encoder1.conv.conv_block.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2075079438032 -> 2075079538912
	2075079538912 [label=AccumulateGrad]
	2075079538864 -> 2075079538768
	2075079438128 [label="encoder1.conv.conv_block.3.bias
 (64)" fillcolor=lightblue]
	2075079438128 -> 2075079538864
	2075079538864 [label=AccumulateGrad]
	2075079538720 -> 2075079538672
	2075079438224 [label="encoder1.conv.conv_block.4.weight
 (64)" fillcolor=lightblue]
	2075079438224 -> 2075079538720
	2075079538720 [label=AccumulateGrad]
	2075079538576 -> 2075079538672
	2075079438320 [label="encoder1.conv.conv_block.4.bias
 (64)" fillcolor=lightblue]
	2075079438320 -> 2075079538576
	2075079538576 [label=AccumulateGrad]
	2075079538384 -> 2075079538240
	2075079438800 [label="encoder2.conv.conv_block.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2075079438800 -> 2075079538384
	2075079538384 [label=AccumulateGrad]
	2075079538336 -> 2075079538240
	2075079438896 [label="encoder2.conv.conv_block.0.bias
 (128)" fillcolor=lightblue]
	2075079438896 -> 2075079538336
	2075079538336 [label=AccumulateGrad]
	2075079538192 -> 2075079538144
	2075079438992 [label="encoder2.conv.conv_block.1.weight
 (128)" fillcolor=lightblue]
	2075079438992 -> 2075079538192
	2075079538192 [label=AccumulateGrad]
	2075079538048 -> 2075079538144
	2075079439088 [label="encoder2.conv.conv_block.1.bias
 (128)" fillcolor=lightblue]
	2075079439088 -> 2075079538048
	2075079538048 [label=AccumulateGrad]
	2075079537904 -> 2075079537760
	2075079439472 [label="encoder2.conv.conv_block.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2075079439472 -> 2075079537904
	2075079537904 [label=AccumulateGrad]
	2075079537856 -> 2075079537760
	2075079439568 [label="encoder2.conv.conv_block.3.bias
 (128)" fillcolor=lightblue]
	2075079439568 -> 2075079537856
	2075079537856 [label=AccumulateGrad]
	2075079537712 -> 2075079537664
	2075079439664 [label="encoder2.conv.conv_block.4.weight
 (128)" fillcolor=lightblue]
	2075079439664 -> 2075079537712
	2075079537712 [label=AccumulateGrad]
	2075079537568 -> 2075079537664
	2075079439760 [label="encoder2.conv.conv_block.4.bias
 (128)" fillcolor=lightblue]
	2075079439760 -> 2075079537568
	2075079537568 [label=AccumulateGrad]
	2075079537376 -> 2075079537232
	2075079440144 [label="encoder3.conv.conv_block.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2075079440144 -> 2075079537376
	2075079537376 [label=AccumulateGrad]
	2075079537328 -> 2075079537232
	2075079440240 [label="encoder3.conv.conv_block.0.bias
 (256)" fillcolor=lightblue]
	2075079440240 -> 2075079537328
	2075079537328 [label=AccumulateGrad]
	2075079537184 -> 2075079537136
	2075079440336 [label="encoder3.conv.conv_block.1.weight
 (256)" fillcolor=lightblue]
	2075079440336 -> 2075079537184
	2075079537184 [label=AccumulateGrad]
	2075079537040 -> 2075079537136
	2075079440432 [label="encoder3.conv.conv_block.1.bias
 (256)" fillcolor=lightblue]
	2075079440432 -> 2075079537040
	2075079537040 [label=AccumulateGrad]
	2075079536896 -> 2075079536752
	2075079440816 [label="encoder3.conv.conv_block.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2075079440816 -> 2075079536896
	2075079536896 [label=AccumulateGrad]
	2075079536848 -> 2075079536752
	2075079440912 [label="encoder3.conv.conv_block.3.bias
 (256)" fillcolor=lightblue]
	2075079440912 -> 2075079536848
	2075079536848 [label=AccumulateGrad]
	2075079536704 -> 2075079536656
	2075079441008 [label="encoder3.conv.conv_block.4.weight
 (256)" fillcolor=lightblue]
	2075079441008 -> 2075079536704
	2075079536704 [label=AccumulateGrad]
	2075079536560 -> 2075079536656
	2075079441104 [label="encoder3.conv.conv_block.4.bias
 (256)" fillcolor=lightblue]
	2075079441104 -> 2075079536560
	2075079536560 [label=AccumulateGrad]
	2075079536368 -> 2075079536224
	2075079441488 [label="encoder4.conv.conv_block.0.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2075079441488 -> 2075079536368
	2075079536368 [label=AccumulateGrad]
	2075079536320 -> 2075079536224
	2075079441584 [label="encoder4.conv.conv_block.0.bias
 (512)" fillcolor=lightblue]
	2075079441584 -> 2075079536320
	2075079536320 [label=AccumulateGrad]
	2075079536176 -> 2075079536128
	2075079441680 [label="encoder4.conv.conv_block.1.weight
 (512)" fillcolor=lightblue]
	2075079441680 -> 2075079536176
	2075079536176 [label=AccumulateGrad]
	2075079536032 -> 2075079536128
	2075079441776 [label="encoder4.conv.conv_block.1.bias
 (512)" fillcolor=lightblue]
	2075079441776 -> 2075079536032
	2075079536032 [label=AccumulateGrad]
	2075079535888 -> 2075079535744
	2075079442160 [label="encoder4.conv.conv_block.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2075079442160 -> 2075079535888
	2075079535888 [label=AccumulateGrad]
	2075079535792 -> 2075079535744
	2075079442256 [label="encoder4.conv.conv_block.3.bias
 (512)" fillcolor=lightblue]
	2075079442256 -> 2075079535792
	2075079535792 [label=AccumulateGrad]
	2075079535696 -> 2075079535648
	2075079442352 [label="encoder4.conv.conv_block.4.weight
 (512)" fillcolor=lightblue]
	2075079442352 -> 2075079535696
	2075079535696 [label=AccumulateGrad]
	2075079535552 -> 2075079535648
	2075079688272 [label="encoder4.conv.conv_block.4.bias
 (512)" fillcolor=lightblue]
	2075079688272 -> 2075079535552
	2075079535552 [label=AccumulateGrad]
	2075079535360 -> 2075079535216
	2075079688656 [label="bottleneck.conv_block.0.weight
 (1024, 512, 3, 3)" fillcolor=lightblue]
	2075079688656 -> 2075079535360
	2075079535360 [label=AccumulateGrad]
	2075079535312 -> 2075079535216
	2075079688752 [label="bottleneck.conv_block.0.bias
 (1024)" fillcolor=lightblue]
	2075079688752 -> 2075079535312
	2075079535312 [label=AccumulateGrad]
	2075079535168 -> 2075079535120
	2075079688848 [label="bottleneck.conv_block.1.weight
 (1024)" fillcolor=lightblue]
	2075079688848 -> 2075079535168
	2075079535168 [label=AccumulateGrad]
	2075079535024 -> 2075079535120
	2075079688944 [label="bottleneck.conv_block.1.bias
 (1024)" fillcolor=lightblue]
	2075079688944 -> 2075079535024
	2075079535024 [label=AccumulateGrad]
	2075079534880 -> 2075079534736
	2075079689328 [label="bottleneck.conv_block.3.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	2075079689328 -> 2075079534880
	2075079534880 [label=AccumulateGrad]
	2075079534832 -> 2075079534736
	2075079689424 [label="bottleneck.conv_block.3.bias
 (1024)" fillcolor=lightblue]
	2075079689424 -> 2075079534832
	2075079534832 [label=AccumulateGrad]
	2075079534688 -> 2075079534640
	2075079689520 [label="bottleneck.conv_block.4.weight
 (1024)" fillcolor=lightblue]
	2075079689520 -> 2075079534688
	2075079534688 [label=AccumulateGrad]
	2075079534448 -> 2075079534640
	2075079689616 [label="bottleneck.conv_block.4.bias
 (1024)" fillcolor=lightblue]
	2075079689616 -> 2075079534448
	2075079534448 [label=AccumulateGrad]
	2075079534304 -> 2075079534208
	2075079690000 [label="decoder1.upconv.weight
 (1024, 512, 2, 2)" fillcolor=lightblue]
	2075079690000 -> 2075079534304
	2075079534304 [label=AccumulateGrad]
	2075079534256 -> 2075079534208
	2075079690096 [label="decoder1.upconv.bias
 (512)" fillcolor=lightblue]
	2075079690096 -> 2075079534256
	2075079534256 [label=AccumulateGrad]
	2075079534160 -> 2075079534016
	2075079533968 -> 2075079533824
	2075079690192 [label="decoder1.conv.conv_block.0.weight
 (512, 1024, 3, 3)" fillcolor=lightblue]
	2075079690192 -> 2075079533968
	2075079533968 [label=AccumulateGrad]
	2075079533920 -> 2075079533824
	2075079690288 [label="decoder1.conv.conv_block.0.bias
 (512)" fillcolor=lightblue]
	2075079690288 -> 2075079533920
	2075079533920 [label=AccumulateGrad]
	2075079533776 -> 2075079533728
	2075079690384 [label="decoder1.conv.conv_block.1.weight
 (512)" fillcolor=lightblue]
	2075079690384 -> 2075079533776
	2075079533776 [label=AccumulateGrad]
	2075079533632 -> 2075079533728
	2075079690480 [label="decoder1.conv.conv_block.1.bias
 (512)" fillcolor=lightblue]
	2075079690480 -> 2075079533632
	2075079533632 [label=AccumulateGrad]
	2075079533488 -> 2075079533344
	2075079690864 [label="decoder1.conv.conv_block.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2075079690864 -> 2075079533488
	2075079533488 [label=AccumulateGrad]
	2075079533440 -> 2075079533344
	2075079690960 [label="decoder1.conv.conv_block.3.bias
 (512)" fillcolor=lightblue]
	2075079690960 -> 2075079533440
	2075079533440 [label=AccumulateGrad]
	2075079533296 -> 2075079533248
	2075079691056 [label="decoder1.conv.conv_block.4.weight
 (512)" fillcolor=lightblue]
	2075079691056 -> 2075079533296
	2075079533296 [label=AccumulateGrad]
	2075079533152 -> 2075079533248
	2075079691152 [label="decoder1.conv.conv_block.4.bias
 (512)" fillcolor=lightblue]
	2075079691152 -> 2075079533152
	2075079533152 [label=AccumulateGrad]
	2075079533008 -> 2075079532912
	2075079691536 [label="decoder2.upconv.weight
 (512, 256, 2, 2)" fillcolor=lightblue]
	2075079691536 -> 2075079533008
	2075079533008 [label=AccumulateGrad]
	2075079532960 -> 2075079532912
	2075079691632 [label="decoder2.upconv.bias
 (256)" fillcolor=lightblue]
	2075079691632 -> 2075079532960
	2075079532960 [label=AccumulateGrad]
	2075079532864 -> 2075079532720
	2075079532672 -> 2075079532528
	2075079691728 [label="decoder2.conv.conv_block.0.weight
 (256, 512, 3, 3)" fillcolor=lightblue]
	2075079691728 -> 2075079532672
	2075079532672 [label=AccumulateGrad]
	2075079532624 -> 2075079532528
	2075079691824 [label="decoder2.conv.conv_block.0.bias
 (256)" fillcolor=lightblue]
	2075079691824 -> 2075079532624
	2075079532624 [label=AccumulateGrad]
	2075079532480 -> 2075079532432
	2075079691920 [label="decoder2.conv.conv_block.1.weight
 (256)" fillcolor=lightblue]
	2075079691920 -> 2075079532480
	2075079532480 [label=AccumulateGrad]
	2075079532336 -> 2075079532432
	2075079692016 [label="decoder2.conv.conv_block.1.bias
 (256)" fillcolor=lightblue]
	2075079692016 -> 2075079532336
	2075079532336 [label=AccumulateGrad]
	2075079532192 -> 2075079532048
	2075079692400 [label="decoder2.conv.conv_block.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2075079692400 -> 2075079532192
	2075079532192 [label=AccumulateGrad]
	2075079532144 -> 2075079532048
	2075079692496 [label="decoder2.conv.conv_block.3.bias
 (256)" fillcolor=lightblue]
	2075079692496 -> 2075079532144
	2075079532144 [label=AccumulateGrad]
	2075079532000 -> 2075079531952
	2075079692592 [label="decoder2.conv.conv_block.4.weight
 (256)" fillcolor=lightblue]
	2075079692592 -> 2075079532000
	2075079532000 [label=AccumulateGrad]
	2075079531856 -> 2075079531952
	2075079692688 [label="decoder2.conv.conv_block.4.bias
 (256)" fillcolor=lightblue]
	2075079692688 -> 2075079531856
	2075079531856 [label=AccumulateGrad]
	2075079531712 -> 2075079531616
	2075079693072 [label="decoder3.upconv.weight
 (256, 128, 2, 2)" fillcolor=lightblue]
	2075079693072 -> 2075079531712
	2075079531712 [label=AccumulateGrad]
	2075079531664 -> 2075079531616
	2075079693168 [label="decoder3.upconv.bias
 (128)" fillcolor=lightblue]
	2075079693168 -> 2075079531664
	2075079531664 [label=AccumulateGrad]
	2075079531568 -> 2075079531424
	2075079531376 -> 2075079531232
	2075079693264 [label="decoder3.conv.conv_block.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	2075079693264 -> 2075079531376
	2075079531376 [label=AccumulateGrad]
	2075079531328 -> 2075079531232
	2075079693360 [label="decoder3.conv.conv_block.0.bias
 (128)" fillcolor=lightblue]
	2075079693360 -> 2075079531328
	2075079531328 [label=AccumulateGrad]
	2075079531184 -> 2075079531136
	2075079693456 [label="decoder3.conv.conv_block.1.weight
 (128)" fillcolor=lightblue]
	2075079693456 -> 2075079531184
	2075079531184 [label=AccumulateGrad]
	2075079531040 -> 2075079531136
	2075079693552 [label="decoder3.conv.conv_block.1.bias
 (128)" fillcolor=lightblue]
	2075079693552 -> 2075079531040
	2075079531040 [label=AccumulateGrad]
	2075079530896 -> 2075079530752
	2075079693936 [label="decoder3.conv.conv_block.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2075079693936 -> 2075079530896
	2075079530896 [label=AccumulateGrad]
	2075079530848 -> 2075079530752
	2075079694032 [label="decoder3.conv.conv_block.3.bias
 (128)" fillcolor=lightblue]
	2075079694032 -> 2075079530848
	2075079530848 [label=AccumulateGrad]
	2075079530704 -> 2075079530656
	2075079694128 [label="decoder3.conv.conv_block.4.weight
 (128)" fillcolor=lightblue]
	2075079694128 -> 2075079530704
	2075079530704 [label=AccumulateGrad]
	2075079530560 -> 2075079530656
	2075079694224 [label="decoder3.conv.conv_block.4.bias
 (128)" fillcolor=lightblue]
	2075079694224 -> 2075079530560
	2075079530560 [label=AccumulateGrad]
	2075079530416 -> 2075079530320
	2075079694608 [label="decoder4.upconv.weight
 (128, 64, 2, 2)" fillcolor=lightblue]
	2075079694608 -> 2075079530416
	2075079530416 [label=AccumulateGrad]
	2075079530368 -> 2075079530320
	2075079694704 [label="decoder4.upconv.bias
 (64)" fillcolor=lightblue]
	2075079694704 -> 2075079530368
	2075079530368 [label=AccumulateGrad]
	2075079530272 -> 2075079528352
	2075079528400 -> 2075079528592
	2075079694800 [label="decoder4.conv.conv_block.0.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	2075079694800 -> 2075079528400
	2075079528400 [label=AccumulateGrad]
	2075079528496 -> 2075079528592
	2075079694896 [label="decoder4.conv.conv_block.0.bias
 (64)" fillcolor=lightblue]
	2075079694896 -> 2075079528496
	2075079528496 [label=AccumulateGrad]
	2075079528640 -> 2075079528784
	2075079694992 [label="decoder4.conv.conv_block.1.weight
 (64)" fillcolor=lightblue]
	2075079694992 -> 2075079528640
	2075079528640 [label=AccumulateGrad]
	2075079528880 -> 2075079528784
	2075079695088 [label="decoder4.conv.conv_block.1.bias
 (64)" fillcolor=lightblue]
	2075079695088 -> 2075079528880
	2075079528880 [label=AccumulateGrad]
	2075079529024 -> 2075079529312
	2075079695472 [label="decoder4.conv.conv_block.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2075079695472 -> 2075079529024
	2075079529024 [label=AccumulateGrad]
	2075079529072 -> 2075079529312
	2075079695568 [label="decoder4.conv.conv_block.3.bias
 (64)" fillcolor=lightblue]
	2075079695568 -> 2075079529072
	2075079529072 [label=AccumulateGrad]
	2075079529360 -> 2075079529648
	2075079695664 [label="decoder4.conv.conv_block.4.weight
 (64)" fillcolor=lightblue]
	2075079695664 -> 2075079529360
	2075079529360 [label=AccumulateGrad]
	2075079529600 -> 2075079529648
	2075079695760 [label="decoder4.conv.conv_block.4.bias
 (64)" fillcolor=lightblue]
	2075079695760 -> 2075079529600
	2075079529600 [label=AccumulateGrad]
	2075079529504 -> 2075079529888
	2075079696144 [label="final_layer.weight
 (1, 64, 1, 1)" fillcolor=lightblue]
	2075079696144 -> 2075079529504
	2075079529504 [label=AccumulateGrad]
	2075079529552 -> 2075079529888
	2075079696240 [label="final_layer.bias
 (1)" fillcolor=lightblue]
	2075079696240 -> 2075079529552
	2075079529552 [label=AccumulateGrad]
	2075079529888 -> 2075079700368
}
